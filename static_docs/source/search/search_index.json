{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Finetrainers is a work-in-progress library to support (accessible) training of diffusion models and various commonly used training algorithms.</p>"},{"location":"#features","title":"Features","text":"<p>DDP, FSDP-2 &amp; HSDP support for all models LoRA and full-rank finetuning; Conditional Control training Memory-efficient single-GPU training Multiple attention backends supported - flash, flex, sage, xformers (see attention docs) Auto-detection of commonly used dataset formats Combined image/video datasets, multiple chainable local/remote datasets, multi-resolution bucketing &amp; more Memory-efficient precomputation support with/without on-the-fly precomputation for large scale datasets Standardized model specification format for training arbitrary models Fake FP8 training (QAT upcoming!)</p>"},{"location":"_NOTES_FOR_FUTURE_ME/","title":"Notes for Future Me","text":"<p>![NOTE] This doc page is intended for developers and contributors.</p> <p>FSDP dump: - https://pytorch.org/docs/stable/notes/fsdp.html#fsdp-notes - https://github.com/pytorch/pytorch/issues/114299 - Using FSDP1 requires that all FSDP flat parameters are of the same dtype. For LoRA training, we default lora parameters to fp32 and transformer parameters to dtype chosen by user. There seems to be no easy workaround than performing lora training in same dtype. - https://github.com/pytorch/pytorch/issues/100945 - https://github.com/pytorch/torchtune/blob/9b3836028fd0b48f593ea43474b86880c49a4d74/recipes/lora_finetune_distributed.py - https://github.com/KellerJordan/modded-nanogpt/pull/68 - https://github.com/pytorch/pytorch/pull/125394: monkey-patch method for FSDP pre/post-hooks to be triggered for method other than <code>forward</code> - https://github.com/pytorch/pytorch/pull/127786: - https://github.com/pytorch/pytorch/pull/130949: - Sanity saver: create optimizers after parallelizing/activation-checkpointing models</p> <p>DTensor: - https://github.com/pytorch/pytorch/issues/88838 - https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_parallelize_api.py</p>"},{"location":"args/","title":"Arguments","text":"<p>This document lists all the arguments that can be passed to the <code>train.py</code> script. For more information, please take a look at the <code>finetrainers/args.py</code> file.</p>"},{"location":"args/#table-of-contents","title":"Table of contents","text":"<ul> <li>General arguments</li> <li>SFT training arguments</li> <li>Control training arguments</li> </ul>"},{"location":"args/#general","title":"General","text":"<pre><code>PARALLEL ARGUMENTS\n------------------\nparallel_backend (`str`, defaults to `accelerate`):\n    The parallel backend to use for training. Choose between ['accelerate', 'ptd'].\npp_degree (`int`, defaults to `1`):\n    The degree of pipeline parallelism.\ndp_degree (`int`, defaults to `1`):\n    The degree of data parallelism (number of model replicas).\ndp_shards (`int`, defaults to `-1`):\n    The number of data parallel shards (number of model partitions).\ncp_degree (`int`, defaults to `1`):\n    The degree of context parallelism.\n</code></pre> <pre><code>MODEL ARGUMENTS\n---------------\nmodel_name (`str`):\n    Name of model to train. To get a list of models, run `python train.py --list_models`.\npretrained_model_name_or_path (`str`):\n    Path to pretrained model or model identifier from https://huggingface.co/models. The model should be\n    loadable based on specified `model_name`.\nrevision (`str`, defaults to `None`):\n    If provided, the model will be loaded from a specific branch of the model repository.\nvariant (`str`, defaults to `None`):\n    Variant of model weights to use. Some models provide weight variants, such as `fp16`, to reduce disk\n    storage requirements.\ncache_dir (`str`, defaults to `None`):\n    The directory where the downloaded models and datasets will be stored, or loaded from.\ntokenizer_id (`str`, defaults to `None`):\n    Identifier for the tokenizer model. This is useful when using a different tokenizer than the default from `pretrained_model_name_or_path`.\ntokenizer_2_id (`str`, defaults to `None`):\n    Identifier for the second tokenizer model. This is useful when using a different tokenizer than the default from `pretrained_model_name_or_path`.\ntokenizer_3_id (`str`, defaults to `None`):\n    Identifier for the third tokenizer model. This is useful when using a different tokenizer than the default from `pretrained_model_name_or_path`.\ntext_encoder_id (`str`, defaults to `None`):\n    Identifier for the text encoder model. This is useful when using a different text encoder than the default from `pretrained_model_name_or_path`.\ntext_encoder_2_id (`str`, defaults to `None`):\n    Identifier for the second text encoder model. This is useful when using a different text encoder than the default from `pretrained_model_name_or_path`.\ntext_encoder_3_id (`str`, defaults to `None`):\n    Identifier for the third text encoder model. This is useful when using a different text encoder than the default from `pretrained_model_name_or_path`.\ntransformer_id (`str`, defaults to `None`):\n    Identifier for the transformer model. This is useful when using a different transformer model than the default from `pretrained_model_name_or_path`.\nvae_id (`str`, defaults to `None`):\n    Identifier for the VAE model. This is useful when using a different VAE model than the default from `pretrained_model_name_or_path`.\ntext_encoder_dtype (`torch.dtype`, defaults to `torch.bfloat16`):\n    Data type for the text encoder when generating text embeddings.\ntext_encoder_2_dtype (`torch.dtype`, defaults to `torch.bfloat16`):\n    Data type for the text encoder 2 when generating text embeddings.\ntext_encoder_3_dtype (`torch.dtype`, defaults to `torch.bfloat16`):\n    Data type for the text encoder 3 when generating text embeddings.\ntransformer_dtype (`torch.dtype`, defaults to `torch.bfloat16`):\n    Data type for the transformer model.\nvae_dtype (`torch.dtype`, defaults to `torch.bfloat16`):\n    Data type for the VAE model.\nlayerwise_upcasting_modules (`List[str]`, defaults to `[]`):\n    Modules that should have fp8 storage weights but higher precision computation. Choose between ['transformer'].\nlayerwise_upcasting_storage_dtype (`torch.dtype`, defaults to `float8_e4m3fn`):\n    Data type for the layerwise upcasting storage. Choose between ['float8_e4m3fn', 'float8_e5m2'].\nlayerwise_upcasting_skip_modules_pattern (`List[str]`, defaults to `[\"patch_embed\", \"pos_embed\", \"x_embedder\", \"context_embedder\", \"^proj_in$\", \"^proj_out$\", \"norm\"]`):\n    Modules to skip for layerwise upcasting. Layers such as normalization and modulation, when casted to fp8 precision\n    naively (as done in layerwise upcasting), can lead to poorer training and inference quality. We skip these layers\n    by default, and recommend adding more layers to the default list based on the model architecture.\ncompile_modules (`List[str]`, defaults to `[]`):\n    Modules that should be regionally compiled with `torch.compile`.\ncompile_scopes (`str`, defaults to `None`):\n    The scope of compilation for each `--compile_modules`. Choose between ['regional', 'full']. Must have the same length as\n    `--compile_modules`. If `None`, will default to `regional` for all modules.\n</code></pre> <pre><code>DATASET ARGUMENTS\n-----------------\ndataset_config (`str`):\n    File to a dataset file containing information about training data. This file can contain information about one or\n    more datasets in JSON format. The file must have a key called \"datasets\", which is a list of dictionaries. Each\n    dictionary must contain the following keys:\n        - \"data_root\": (`str`)\n            The root directory containing the dataset. This parameter must be provided if `dataset_file` is not provided.\n        - \"dataset_file\": (`str`)\n            Path to a CSV/JSON/JSONL/PARQUET/ARROW/HF_HUB_DATASET file containing metadata for training. This parameter\n            must be provided if `data_root` is not provided.\n        - \"dataset_type\": (`str`)\n            Type of dataset. Choose between ['image', 'video'].\n        - \"id_token\": (`str`)\n            Identifier token appended to the start of each prompt if provided. This is useful for LoRA-type training\n            for single subject/concept/style training, but is not necessary.\n        - \"image_resolution_buckets\": (`List[Tuple[int, int]]`)\n            Resolution buckets for image. This should be a list of tuples containing 2 values, where each tuple\n            represents the resolution (height, width). All images will be resized to the nearest bucket resolution.\n            This parameter must be provided if `dataset_type` is 'image'.\n        - \"video_resolution_buckets\": (`List[Tuple[int, int, int]]`)\n            Resolution buckets for video. This should be a list of tuples containing 3 values, where each tuple\n            represents the resolution (num_frames, height, width). All videos will be resized to the nearest bucket\n            resolution. This parameter must be provided if `dataset_type` is 'video'.\n        - \"reshape_mode\": (`str`)\n            All input images/videos are reshaped using this mode. Choose between the following:\n            [\"center_crop\", \"random_crop\", \"bicubic\"].\n        - \"remove_common_llm_caption_prefixes\": (`boolean`)\n            Whether or not to remove common LLM caption prefixes. See `~constants.py` for the list of common prefixes.\ndataset_shuffle_buffer_size (`int`, defaults to `1`):\n    The buffer size for shuffling the dataset. This is useful for shuffling the dataset before training. The default\n    value of `1` means that the dataset will not be shuffled.\nenable_precomputation (`bool`, defaults to `False`):\n    Whether or not to precompute the embeddings for the dataset. This is useful for faster training. If set to `True`,\n    the embeddings will be precomputed and saved to disk and loaded as required.\nprecomputation_items (`int`, defaults to `512`):\n    Number of data samples to precompute at once for memory-efficient training. The higher this value,\n    the more disk memory will be used to save the precomputed samples (conditions and latents).\nprecomputation_dir (`str`, defaults to `None`):\n    The directory where the precomputed samples will be stored. If not provided, the precomputed samples\n    will be stored in a temporary directory of the output directory.\nprecomputation_once (`bool`, defaults to `False`):\n    Precompute embeddings from all datasets at once before training. This is useful to save time during training\n    with smaller datasets. If set to `False`, will save disk space by precomputing embeddings on-the-fly during\n    training when required (that is, computing embeddings of more data samples once `precomputation_items` of them\n    have been exhausted across all distributed ranks). Make sure to set `precomputation_items` to a reasonable value\n    in line with the size of your dataset(s).\nprecomputation_reuse (`bool`, defaults to `False`):\n    Reuse precomputed embeddings from previous training runs. This is useful to save time during training\n    with medium/large datasets. By default, old precomputed embeddings that exist in the specified precomputation\n    directory, or default precomputation dir `{output_dir}/precomputed` will be deleted if this is not set to `True`.\n    This flag is ignored if `enable_precomputation` is `False`. The topology of the distributed training run must be\n    the same as the one used to precompute the embeddings for this to work correctly (this limitation will be \n    addressed in the future).\n</code></pre> <pre><code>DATALOADER_ARGUMENTS\n--------------------\nSee https://pytorch.org/docs/stable/data.html for more information.\n\ndataloader_num_workers (`int`, defaults to `0`):\n    Number of subprocesses to use for data loading. `0` means that the data will be loaded in a blocking manner\n    on the main process.\npin_memory (`bool`, defaults to `False`):\n    Whether or not to use the pinned memory setting in PyTorch dataloader. This is useful for faster data loading.\n</code></pre> <pre><code>DIFFUSION ARGUMENTS\n-------------------\nflow_resolution_shifting (`bool`, defaults to `False`):\n    Resolution-dependent shifting of timestep schedules.\n    [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206).\n    TODO(aryan): We don't support this yet.\nflow_base_seq_len (`int`, defaults to `256`):\n    Base number of tokens for images/video when applying resolution-dependent shifting.\nflow_max_seq_len (`int`, defaults to `4096`):\n    Maximum number of tokens for images/video when applying resolution-dependent shifting.\nflow_base_shift (`float`, defaults to `0.5`):\n    Base shift for timestep schedules when applying resolution-dependent shifting.\nflow_max_shift (`float`, defaults to `1.15`):\n    Maximum shift for timestep schedules when applying resolution-dependent shifting.\nflow_shift (`float`, defaults to `1.0`):\n    Instead of training with uniform/logit-normal sigmas, shift them as (shift * sigma) / (1 + (shift - 1) * sigma).\n    Setting it higher is helpful when trying to train models for high-resolution generation or to produce better\n    samples in lower number of inference steps.\nflow_weighting_scheme (`str`, defaults to `none`):\n    We default to the \"none\" weighting scheme for uniform sampling and uniform loss.\n    Choose between ['sigma_sqrt', 'logit_normal', 'mode', 'cosmap', 'none'].\nflow_logit_mean (`float`, defaults to `0.0`):\n    Mean to use when using the `'logit_normal'` weighting scheme.\nflow_logit_std (`float`, defaults to `1.0`):\n    Standard deviation to use when using the `'logit_normal'` weighting scheme.\nflow_mode_scale (`float`, defaults to `1.29`):\n    Scale of mode weighting scheme. Only effective when using the `'mode'` as the `weighting_scheme`.\n</code></pre> <pre><code>TRAINING ARGUMENTS\n------------------\ntraining_type (`str`, defaults to `None`):\n    Type of training to perform. Choose between ['lora'].\nseed (`int`, defaults to `42`):\n    A seed for reproducible training.\nbatch_size (`int`, defaults to `1`):\n    Per-device batch size.\ntrain_steps (`int`, defaults to `1000`):\n    Total number of training steps to perform.\nmax_data_samples (`int`, defaults to `2**64`):\n    Maximum number of data samples observed during training training. If lesser than that required by `train_steps`,\n    the training will stop early.\ngradient_accumulation_steps (`int`, defaults to `1`):\n    Number of gradients steps to accumulate before performing an optimizer step.\ngradient_checkpointing (`bool`, defaults to `False`):\n    Whether or not to use gradient/activation checkpointing to save memory at the expense of slower\n    backward pass.\ncheckpointing_steps (`int`, defaults to `500`):\n    Save a checkpoint of the training state every X training steps. These checkpoints can be used both\n    as final checkpoints in case they are better than the last checkpoint, and are also suitable for\n    resuming training using `resume_from_checkpoint`.\ncheckpointing_limit (`int`, defaults to `None`):\n    Max number of checkpoints to store.\nresume_from_checkpoint (`str`, defaults to `None`):\n    Whether training should be resumed from a previous checkpoint. Use a path saved by `checkpointing_steps`,\n    or `\"latest\"` to automatically select the last available checkpoint.\n</code></pre> <pre><code>OPTIMIZER ARGUMENTS\n-------------------\noptimizer (`str`, defaults to `adamw`):\n    The optimizer type to use. Choose between the following:\n        - Torch optimizers: [\"adam\", \"adamw\"]\n        - Bitsandbytes optimizers: [\"adam-bnb\", \"adamw-bnb\", \"adam-bnb-8bit\", \"adamw-bnb-8bit\"]\nlr (`float`, defaults to `1e-4`):\n    Initial learning rate (after the potential warmup period) to use.\nlr_scheduler (`str`, defaults to `cosine_with_restarts`):\n    The scheduler type to use. Choose between ['linear', 'cosine', 'cosine_with_restarts', 'polynomial',\n    'constant', 'constant_with_warmup'].\nlr_warmup_steps (`int`, defaults to `500`):\n    Number of steps for the warmup in the lr scheduler.\nlr_num_cycles (`int`, defaults to `1`):\n    Number of hard resets of the lr in cosine_with_restarts scheduler.\nlr_power (`float`, defaults to `1.0`):\n    Power factor of the polynomial scheduler.\nbeta1 (`float`, defaults to `0.9`):\nbeta2 (`float`, defaults to `0.95`):\nbeta3 (`float`, defaults to `0.999`):\nweight_decay (`float`, defaults to `0.0001`):\n    Penalty for large weights in the model.\nepsilon (`float`, defaults to `1e-8`):\n    Small value to avoid division by zero in the optimizer.\nmax_grad_norm (`float`, defaults to `1.0`):\n    Maximum gradient norm to clip the gradients.\n</code></pre> <pre><code>VALIDATION ARGUMENTS\n--------------------\nvalidation_dataset_file (`str`, defaults to `None`):\n    Path to a CSV/JSON/PARQUET/ARROW file containing information for validation. The file must contain atleast the\n    \"caption\" column. Other columns such as \"image_path\" and \"video_path\" can be provided too. If provided, \"image_path\"\n    will be used to load a PIL.Image.Image and set the \"image\" key in the sample dictionary. Similarly, \"video_path\"\n    will be used to load a List[PIL.Image.Image] and set the \"video\" key in the sample dictionary.\n    The validation dataset file may contain other attributes specific to inference/validation such as:\n        - \"height\" and \"width\" and \"num_frames\": Resolution\n        - \"num_inference_steps\": Number of inference steps\n        - \"guidance_scale\": Classifier-free Guidance Scale\n        - ... (any number of additional attributes can be provided. The ModelSpecification::validate method will be\n          invoked with the sample dictionary to validate the sample.)\nvalidation_steps (`int`, defaults to `500`):\n    Number of training steps after which a validation step is performed.\nenable_model_cpu_offload (`bool`, defaults to `False`):\n    Whether or not to offload different modeling components to CPU during validation.\n</code></pre> <pre><code>MISCELLANEOUS ARGUMENTS\n-----------------------\ntracker_name (`str`, defaults to `finetrainers`):\n    Name of the tracker/project to use for logging training metrics.\npush_to_hub (`bool`, defaults to `False`):\n    Whether or not to push the model to the Hugging Face Hub.\nhub_token (`str`, defaults to `None`):\n    The API token to use for pushing the model to the Hugging Face Hub.\nhub_model_id (`str`, defaults to `None`):\n    The model identifier to use for pushing the model to the Hugging Face Hub.\noutput_dir (`str`, defaults to `None`):\n    The directory where the model checkpoints and logs will be stored.\nlogging_dir (`str`, defaults to `logs`):\n    The directory where the logs will be stored.\nlogging_steps (`int`, defaults to `1`):\n    Training logs will be tracked every `logging_steps` steps.\nnccl_timeout (`int`, defaults to `1800`):\n    Timeout for the NCCL communication.\nreport_to (`str`, defaults to `wandb`):\n    The name of the logger to use for logging training metrics. Choose between ['wandb'].\nverbose (`int`, defaults to `1`):\n    Whether or not to print verbose logs.\n        - 0: Diffusers/Transformers warning logging on local main process only\n        - 1: Diffusers/Transformers info logging on local main process only\n        - 2: Diffusers/Transformers debug logging on local main process only\n        - 3: Diffusers/Transformers debug logging on all processes\n</code></pre> <pre><code>TORCH CONFIG ARGUMENTS\n----------------------\nallow_tf32 (`bool`, defaults to `False`):\n    Whether or not to allow the use of TF32 matmul on compatible hardware.\nfloat32_matmul_precision (`str`, defaults to `highest`):\n    The precision to use for float32 matmul. Choose between ['highest', 'high', 'medium'].\n</code></pre>"},{"location":"args/#attention-provider","title":"Attention Provider","text":"<p>These arguments are relevant to setting attention provider for different modeling components. The attention providers may be set differently for training and validation/inference.</p> <pre><code>attn_provider_training (`str`, defaults to \"native\"):\n    The attention provider to use for training. Choose between\n    [\n        'flash', 'flash_varlen', 'flex', 'native', '_native_cudnn', '_native_efficient', '_native_flash',\n        '_native_math'\n    ]\nattn_provider_inference (`str`, defaults to \"native\"):\n    The attention provider to use for validation. Choose between\n    [\n        'flash', 'flash_varlen', 'flex', 'native', '_native_cudnn', '_native_efficient', '_native_flash',\n        '_native_math', 'sage', 'sage_varlen', '_sage_qk_int8_pv_fp8_cuda', '_sage_qk_int8_pv_fp8_cuda_sm90',\n        '_sage_qk_int8_pv_fp16_cuda', '_sage_qk_int8_pv_fp16_triton', 'xformers'\n    ]\n</code></pre>"},{"location":"args/#sft-training","title":"SFT training","text":"<p>If using <code>--training_type lora</code>, these arguments can be specified.</p> <pre><code>rank (int):\n    Rank of the low rank approximation.\nlora_alpha (int):\n    The lora_alpha parameter to compute scaling factor (lora_alpha / rank) for low-rank matrices.\ntarget_modules (`str` or `List[str]`):\n    Target modules for the low rank approximation. Can be a regex string or a list of regex strings.\n</code></pre> <p>No additional arguments are required for <code>--training_type full-finetune</code>.</p>"},{"location":"args/#control-training","title":"Control training","text":"<p>If using <code>--training_type control-lora</code>, these arguments can be specified.</p> <pre><code>control_type (`str`, defaults to `\"canny\"`):\n    Control type for the low rank approximation matrices. Can be \"canny\", \"custom\".\nrank (int, defaults to `64`):\n    Rank of the low rank approximation matrix.\nlora_alpha (int, defaults to `64`):\n    The lora_alpha parameter to compute scaling factor (lora_alpha / rank) for low-rank matrices.\ntarget_modules (`str` or `List[str]`, defaults to `\"(transformer_blocks|single_transformer_blocks).*(to_q|to_k|to_v|to_out.0|ff.net.0.proj|ff.net.2)\"`):\n    Target modules for the low rank approximation matrices. Can be a regex string or a list of regex strings.\ntrain_qk_norm (`bool`, defaults to `False`):\n    Whether to train the QK normalization layers.\nframe_conditioning_type (`str`, defaults to `\"full\"`):\n    Type of frame conditioning. Can be \"index\", \"prefix\", \"random\", \"first_and_last\", or \"full\".\nframe_conditioning_index (int, defaults to `0`):\n    Index of the frame conditioning. Only used if `frame_conditioning_type` is \"index\".\nframe_conditioning_concatenate_mask (`bool`, defaults to `False`):\n    Whether to concatenate the frame mask with the latents across channel dim.\n</code></pre> <p>If using <code>--training_type control-full-finetune</code>, these arguments can be specified.</p> <pre><code>control_type (`str`, defaults to `\"canny\"`):\n    Control type for the low rank approximation matrices. Can be \"canny\", \"custom\".\ntrain_qk_norm (`bool`, defaults to `False`):\n    Whether to train the QK normalization layers.\nframe_conditioning_type (`str`, defaults to `\"index\"`):\n    Type of frame conditioning. Can be \"index\", \"prefix\", \"random\", \"first_and_last\", or \"full\".\nframe_conditioning_index (int, defaults to `0`):\n    Index of the frame conditioning. Only used if `frame_conditioning_type` is \"index\".\nframe_conditioning_concatenate_mask (`bool`, defaults to `False`):\n    Whether to concatenate the frame mask with the latents across channel dim.\n</code></pre>"},{"location":"environment/","title":"Environment","text":"<p>Finetrainers has only been widely tested with the following environment (output obtained by running <code>diffusers-cli env</code>):</p> <pre><code>- \ud83e\udd17 Diffusers version: 0.33.0.dev0\n- Platform: Linux-5.4.0-166-generic-x86_64-with-glibc2.31\n- Running on Google Colab?: No\n- Python version: 3.10.14\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\n- Flax version (CPU?/GPU?/TPU?): 0.8.5 (cpu)\n- Jax version: 0.4.31\n- JaxLib version: 0.4.31\n- Huggingface_hub version: 0.28.1\n- Transformers version: 4.48.0.dev0\n- Accelerate version: 1.1.0.dev0\n- PEFT version: 0.14.1.dev0\n- Bitsandbytes version: 0.43.3\n- Safetensors version: 0.4.5\n- xFormers version: not installed\n- Accelerator: NVIDIA A100-SXM4-80GB, 81920 MiB\nNVIDIA A100-SXM4-80GB, 81920 MiB\nNVIDIA A100-SXM4-80GB, 81920 MiB\nNVIDIA DGX Display, 4096 MiB\nNVIDIA A100-SXM4-80GB, 81920 MiB\n</code></pre> <p>Other versions of dependencies may or may not work as expected. We would like to make finetrainers work on a wider range of environments, but due to the complexity of testing at the early stages of development, we are unable to do so. The long term goals include compatibility with most pytorch versions on CUDA, MPS, ROCm and XLA devices.</p>"},{"location":"environment/#configuration","title":"Configuration","text":"<p>The following environment variables may be configured to change the default behaviour of finetrainers:</p> <p><code>FINETRAINERS_ATTN_PROVIDER</code>: Sets the default attention provider for training/validation. Defaults to <code>native</code>, as in native PyTorch SDPA. See attention docs for more information. <code>FINETRAINERS_ATTN_CHECKS</code>: Whether or not to run basic sanity checks when using different attention providers. This is useful for debugging but you should leave it disabled for longer training runs. Defaults to <code>\"0\"</code>. Can be set to a truthy env value.</p>"},{"location":"optimizer/","title":"Optimizers","text":"<p>The following optimizers are supported: - torch:   - <code>Adam</code>   - <code>AdamW</code> - bitsandbytes:   - <code>Adam</code>   - <code>AdamW</code>   - <code>Adam8Bit</code>   - <code>AdamW8Bit</code></p> <p>Warning</p> <p>Not all optimizers have been tested with all models/parallel settings. They may or may not work, but this will gradually improve over time.</p>"},{"location":"dataset/","title":"Dataset","text":""},{"location":"dataset/#dataset-preparation","title":"Dataset preparation","text":"<p>Please refer to video-dataset-scripts for a collection of scripts to prepare datasets for training. The scripts are designed to work with the HF datasets library and can be used to prepare datasets for training with <code>finetrainers</code>.</p>"},{"location":"dataset/#training-dataset-format","title":"Training Dataset Format","text":"<p>Dataset loading format support is very limited at the moment. This will be improved in the future. For now, we support the following formats:</p>"},{"location":"dataset/#two-file-format","title":"Two file format","text":"<p>Tip</p> <p>Relevant classes to look for implementation: 1. ImageFileCaptionFileListDataset 2. VideoFileCaptionFileListDataset Supports loading directly from the HF Hub.</p> <p>Your dataset structure should look like this. Running the <code>tree</code> command, you should see something similar to:</p> <pre><code>dataset\n\u251c\u2500\u2500 prompt.txt\n\u251c\u2500\u2500 videos.txt\n\u251c\u2500\u2500 videos\n    \u251c\u2500\u2500 00000.mp4\n    \u251c\u2500\u2500 00001.mp4\n    \u251c\u2500\u2500 ...\n</code></pre> <ul> <li>Make sure that the paths in <code>videos.txt</code> is relative to the <code>dataset</code> directory. The <code>prompt.txt</code> should contain the captions for the videos in the same order as the videos in <code>videos.txt</code>.</li> <li>Supported names for caption file: <code>captions.txt</code>, <code>caption.txt</code>, <code>prompt.txt</code>, <code>prompts.txt</code> (feel free to send PRs to add more common names).</li> <li>Supported names for video file: <code>videos.txt</code>, <code>video.txt</code>, (feel free to send PRs to add more common names).</li> </ul>"},{"location":"dataset/#caption-data-filename-pair-format","title":"Caption-Data filename pair format","text":"<p>[!NOTE] Relevant classes to look for implementation: - ImageCaptionFilePairDataset - VideoCaptionFilePairDataset</p> <p>Does not support loading directly from the HF Hub.</p> <p>Your dataset structure should look like this. Running the <code>tree</code> command, you should see something similar to:</p> <pre><code>dataset\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 a.mp4\n\u251c\u2500\u2500 bkjlaskdjg.txt\n\u251c\u2500\u2500 bkjlaskdjg.mp4\n\u251c\u2500\u2500 ...\n</code></pre> <ul> <li>Each caption file should have a corresponding image/video file with the same name.</li> </ul>"},{"location":"dataset/#csvjsonjsonl-format","title":"CSV/JSON/JSONL format","text":"<ul> <li>Supported names are: <code>metadata.json</code>, <code>metadata.jsonl</code>, <code>metadata.csv</code></li> </ul> <p>[!NOTE] Relevant classes to look for implementation: - ImageFolderDataset - VideoFolderDataset</p> <p>Any dataset loadable via the [\ud83e\udd17 HF datasets] directly should work (not widely tested at the moment): - https://huggingface.co/docs/datasets/v3.3.2/en/image_load#webdataset - https://huggingface.co/docs/datasets/v3.3.2/en/video_load#webdataset</p>"},{"location":"dataset/#webdataset-format","title":"Webdataset format","text":"<p>[!NOTE] Relevant classes to look for implementation: - ImageWebDataset - VideoWebDataset</p> <p>Any dataset loadable via the [\ud83e\udd17 HF datasets] directly should work (not widely tested at the moment). We support the <code>webdataset</code> and <code>webdataset</code> formats.</p>"},{"location":"dataset/#validation-dataset-format","title":"Validation Dataset Format","text":"<p>Arguments related to validation are: - <code>--validation_dataset_file</code>: Path to the validation dataset file. Supported formats are CSV, JSON, JSONL, PARQUET, and ARROW. Note: PARQUET and ARROW have not been tested after a major refactor, but should most likely work. (TODO(aryan): look into this) - <code>--validation_steps</code>: Interval of training steps after which validation should be performed. - <code>--enable_model_cpu_offload</code>: If set, CPU offloading will be enabled during validation. Note that this has not been tested for FSDP, TP, or DDP after a major refactor, but should most likely work for single GPU training,</p> <p>[!IMPORTANT]</p> <p>When using <code>dp_shards &gt; 1</code> or <code>tp_degree &gt; 1</code>, you must make sure that the number of data samples contained is a multiple of <code>dp_shards * tp_degree</code>. If this is not the case, the training will fail due to a NCCL timeout. This will be improved/fixed in the future.</p> <ul> <li>Must contain \"caption\" as a column. If an image must be provided for validation (for example, image-to-video inference), then the \"image_path\" field must be provided. If a video must be provided for validation (for example, video-to-video inference), then the \"video_path\" field must be provided. Other fields like \"num_inference_steps\", \"height\", \"width\", \"num_frames\", and \"frame_rate\" can be provided too but are optional.</li> </ul>"},{"location":"dataset/#csv-example","title":"CSV Example","text":"Click to expand <pre><code>caption,image_path,video_path,num_inference_steps,height,width,num_frames,frame_rate\n\"A black and white animated scene unfolds with an anthropomorphic goat surrounded by musical notes and symbols, suggesting a playful environment. Mickey Mouse appears, leaning forward in curiosity as the goat remains still. The goat then engages with Mickey, who bends down to converse or react. The dynamics shift as Mickey grabs the goat, potentially in surprise or playfulness, amidst a minimalistic background. The scene captures the evolving relationship between the two characters in a whimsical, animated setting, emphasizing their interactions and emotions.\",,\"/raid/aryan/finetrainers-dummy-dataset-disney/a3c275fc2eb0a67168a7c58a6a9adb14.mp4\",50,480,768,49,25\n\"&lt;SECOND_CAPTION&gt;\",,\"/path/to/second.mp4\",50,512,704,161,25\n</code></pre>"},{"location":"dataset/#json-example","title":"JSON Example","text":"<p>Must contain \"data\" field, which should be a list of dictionaries. Each dictionary corresponds to one validation video that will be generated with the selected configuration of generation parameters.</p> Click to expand <pre><code>{\n  \"data\": [\n    {\n      \"caption\": \"A black and white animated scene unfolds with an anthropomorphic goat surrounded by musical notes and symbols, suggesting a playful environment. Mickey Mouse appears, leaning forward in curiosity as the goat remains still. The goat then engages with Mickey, who bends down to converse or react. The dynamics shift as Mickey grabs the goat, potentially in surprise or playfulness, amidst a minimalistic background. The scene captures the evolving relationship between the two characters in a whimsical, animated setting, emphasizing their interactions and emotions.\",\n      \"image_path\": \"\",\n      \"video_path\": \"/raid/aryan/finetrainers-dummy-dataset-disney/a3c275fc2eb0a67168a7c58a6a9adb14.mp4\",\n      \"num_inference_steps\": 50,\n      \"height\": 480,\n      \"width\": 768,\n      \"num_frames\": 49,\n      \"frame_rate\": 25\n    },\n    {\n      \"caption\": \"&lt;SECOND_CAPTION&gt;\",\n      \"image_path\": \"\",\n      \"video_path\": \"/path/to/second.mp4\",\n      \"num_inference_steps\": 50,\n      \"height\": 512,\n      \"width\": 704,\n      \"num_frames\": 161,\n      \"frame_rate\": 25\n    }\n  ]\n}\n</code></pre>"},{"location":"dataset/#understanding-how-datasets-are-loaded","title":"Understanding how datasets are loaded","text":"<p>For memory efficient training, it is important to precompute conditional and latent embeddings. If this is not done, we will need to keep the conditioning models in memory, which can be memory intensive. To avoid this, we implement some abstractions that allow us to do the following efficiently: - Loading datasets - Chaining multiple datasets together - Splitting datasets across data replicas - Preprocessing datasets to user-configured resolution buckets - Precomputing embeddings without exhaustively using too much disk space</p> <p>The following is a high-level overview of how datasets are loaded and preprocessed:</p> <ul> <li>Initially, the dataset is lazy loaded using the HF <code>datasets</code> library. Every dataset is loaded in streaming and infinite mode. This means that the dataset will be loaded indefinitely until some end conditions (e.g. user-configured training steps is completed). Multiple datasets can be chained together. For example, if you only have high resolution data available, but want to perform multi-resolution training at certain lower resolutions too, you would have to perform the resizing manually and create a new copy of the dataset containing multiresolution data. Finetrainers makes this easier by allowing you to specify multiple different, or same, datasets with different resolutions.</li> <li>When chaining multiple different datasets, make sure they are roughly the same size to avoid having smaller datasets repeatedly being used in the training loop. This is because the datasets are loaded in a round-robin fashion. For example, if you have 2 datasets of size 1000 and 2000, the first dataset will be fully seen twice before the second dataset is fully seen once by the model.</li> <li>The dataset is split across data replicas (GPUs groups that perform data parallelism). Each data replica will have a non-overlapping subset of the overall dataset.</li> <li>If multiple datasets have been provided, they will be chained together. Shuffling can also be done to ensure better dataset regularization. This is done by shuffling the iterable datasets in a buffer of user-configured <code>--dataset_shuffle_buffer_size</code>. For small datasets, it is recommended to not shuffle and use the default value of <code>1</code>. For larger datasets, there is a significant overhead the higher this value is set to, so it is recommended to keep it low (&lt; 1000) [this is because we store the data in memory in a not-so-clever way].</li> <li>The dataset is preprocessed to the user-configured resolution buckets. This is done by resizing the images/videos to the specified resolution buckets. This is also necessary for collation when using batch_size &gt; 1.</li> <li>The dataset is precomputed for embeddings and stored to disk. This is done in batches of user-configured <code>--precomputation_items</code> to avoid exhausting disk space. The smaller this value, the more number of times conditioning models will be loaded upon precomputation exhaustion. The larger this value, the more disk space will be used.</li> <li>When data points are required for training, they are loaded from disk on the main process and dispatched to data replicas. [TODO: this needs some improvements to speedup training eventually]</li> </ul>"},{"location":"dataset/#understanding-how-datasets-are-precomputed","title":"Understanding how datasets are precomputed","text":"<p>There are 4 arguments related to precomputation: - <code>--enable_precomputation</code>: If set, precomputation will be enabled. The parameters that follow are only relevant if this flag is set. If this flag is not set, all models will be loaded in memory and training will take place without first precomputing embeddings. - <code>--precomputation_items</code>: The number of data points to precompute and store to disk at a time. This is useful for performing memory-efficient training without exhausting disk space by precomputing embeddings of the entire dataset(s) at once. We default to <code>512</code> data points, but configure this to a lower value for smaller datasets. As training progresses, the precomputed data will be read from disk and dispatched to data replicas. Once all precomputed data has been used, the next batch of data points will be precomputed and stored to disk in a rolling fashion. - <code>--precomputation_dir</code>: The directory where precomputed data will be stored. This is useful for resuming training from a checkpoint, as the precomputed data will be loaded from this directory. If this directory is not provided, the precomputed data will be stored in the <code>--output_dir/precomputed</code>. - <code>--precomputation_once</code>: If you're working with small datasets and want to precompute all embeddings at once, set this flag. This will allow you to train without having to compute embeddings every time the precomputed data is exhausted. Currently, <code>webdataset</code> format loading does not support this feature, and it is also disabled for <code>&gt; 1024</code> data points due to hard coded logic (can be removed manually by users for now). - <code>--precomputation_reuse</code>: If you're working with medium/large-size datasets and want to precompute all embeddings and re-use them across different training runs, make sure to set this flag.</p> <p>Batching is not yet supported for precomputation. This will be added in the future.</p>"},{"location":"dataset/_DEBUG/","title":"Distributed dataset debugging","text":"<p>![NOTE] This doc page is intended for developers and contributors.</p> <p>If the number of samples in the dataset is lower than the number of processes per node, the training will hand indefinitely. I haven't been able to pin down on how this could be fixed due to limited time, but basically: - Start training with <code>--dp_degree 2</code> and <code>torchrun --standalone --nnodes=1 --nproc_per_node=2</code>. This launches training with DDP across 2 ranks. - The dataset has <code>&lt; dp_degree</code> samples - When <code>datasets.distributed.split_dataset_by_node</code> is called, the data is distributed correctly to one rank, but the other rank hangs indefinitely. Due to this edge case, fast tests seem to fail. - For now, we should just use <code>&gt;= dp_degree</code> samples in the test dataset. However, should be fixed in the future.</p> <p>Minimal reproducer:</p> <pre><code>import torch\nimport torch.distributed as dist\nfrom datasets import Dataset\nfrom datasets.distributed import split_dataset_by_node\nfrom torch.utils.data import DataLoader\n\nds = Dataset.from_dict({\"x\": [1]}).to_iterable_dataset()\n\ndist.init_process_group()\nrank, world_size = dist.get_rank(), dist.get_world_size()\nds = split_dataset_by_node(ds, rank=rank,world_size=world_size)\ndl = DataLoader(ds)\n\nexhausted = torch.zeros(world_size, dtype=torch.bool)\n\ndef loop():\n    while True:\n        print(rank, \"hello\", flush=True)\n        yield from dl\n        yield \"end\"\n\nfor x in loop():\n    if x == \"end\":\n        exhausted[rank] = True\n        continue\n    dist.all_reduce(exhausted)\n    if torch.all(exhausted):\n        break\n    print(f\"{rank} {x}\", flush=True)\n</code></pre>"},{"location":"models/","title":"Finetrainers training documentation","text":"<p>This directory contains the training-related specifications for all the models we support in <code>finetrainers</code>. Each model page has: - an example training command - inference example - numbers on memory consumption</p> <p>By default, we don't include any validation-related arguments in the example training commands. To enable validation inference every 500 steps, one can add the following arguments:</p> <pre><code>+ --validation_dataset_file &lt;Path to a CSV/JSON/PARQUET/ARROW&gt; \\\n+ --validation_steps 500\n</code></pre> <p>Arguments for training are documented in the code. For more information, please run <code>python train.py --help</code>.</p>"},{"location":"models/#support-matrix","title":"Support matrix","text":"<p>The following table shows the algorithms supported for training and the models they are supported for:</p> Model SFT Control ControlNet Distillation CogVideoX \ud83e\udd17 \ud83d\ude21 \ud83d\ude21 \ud83d\ude21 CogView4 \ud83e\udd17 \ud83e\udd17 \ud83d\ude21 \ud83d\ude21 Flux \ud83e\udd17 \ud83d\ude21 \ud83d\ude21 \ud83d\ude21 HunyuanVideo \ud83e\udd17 \ud83d\ude21 \ud83d\ude21 \ud83d\ude21 LTX-Video \ud83e\udd17 \ud83d\ude21 \ud83d\ude21 \ud83d\ude21 Wan \ud83e\udd17 \ud83e\udd17 \ud83d\ude21 \ud83d\ude21 <p>For launching SFT Training: - <code>--training_type lora</code>: Trains a new set of low-rank weights of the model, yielding a smaller adapter model. Currently, only LoRA is supported from \ud83e\udd17 PEFT - <code>--training_type full-finetune</code>: Trains the full-rank weights of the model, yielding a full-parameter trained model.</p> <p>For launching Control Training: - <code>--training_type control-lora</code>: Trains lora-rank weights for additional channel-wise concatenated control condition. - <code>--training_type control-full-finetune</code>: Trains the full-rank control conditioned model.</p> <p>Any model architecture loadable in diffusers/transformers for above models can be used for training. For example, SkyReels-T2V is a finetune of HunyuanVideo, which is compatible for continual training out-of-the-box. Custom models can be loaded either by writing your own ModelSpecification or by using the following set of arguments: - <code>--tokenizer_id</code>, <code>--tokenizer_2_id</code>, <code>--tokenizer_3_id</code>: The tokenizers to use for training in conjunction with text encoder conditioning models. - <code>--text_encoder_id</code>, <code>--text_encoder_2_id</code>, <code>--text_encoder_3_id</code>: The text encoder conditioning models. - <code>--transformer_id</code>: The transformer model to use for training. - <code>--vae_id</code>: The VAE model to use for training.</p> <p>The above arguments should take care of most training scenarios. For any custom training scenarios, please use your own implementation of a <code>ModelSpecification</code>. These arguments should be used only if one wants to override the default components loaded from <code>--pretrained_model_name_or_path</code>. Similar to each of these arguments, there exists a set of <code>--&lt;ARG&gt;_dtype</code> argument to specify the precision of each component.</p>"},{"location":"models/#resuming-training","title":"Resuming training","text":"<p>To resume training, the following arguments can be used: - <code>--checkpointing_steps</code>: The interval of training steps that should be completed after which the training state should be saved. - <code>--checkpointing_limit</code>: The maximum number of checkpoints that should be saved at once. If the limit is reached, the oldest checkpoint is purged. - <code>--resume_from_checkpoint &lt;STEP_OR_LATEST&gt;</code>: Can be an integer or the string <code>\"latest\"</code>. If an integer is provided, training will resume from that step if a checkpoint corresponding to it exists. If <code>\"latest\"</code> is provided, training will resume from the latest checkpoint in the <code>--output_dir</code>.</p> <p>[!IMPORTANT] The <code>--resume_from_checkpoint</code> argument is only compatible if the parallel backend and degrees of parallelism are the same from the previous training run. For example, changing <code>--dp_degree 2 --dp_shards 1</code> from past run to <code>--dp_degree 1 --dp_shards 2</code> in current run will not work.</p>"},{"location":"models/#how-do-we-handle-mixed_precision","title":"How do we handle <code>mixed_precision</code>?","text":"<p>The accelerate config files (the ones seen here) that are being supplied while launching training should contain a field called <code>mixed_precision</code> and <code>accelerate</code> makes use of that if specified. We don't let users explicitly pass that from the CLI args because it can be confusing to have <code>transformer_dtype</code> and <code>mixed_precision</code> in the codebase.</p> <p><code>transformer_dtype</code> is the ultimate source of truth for the precision to be used when training. It will also most likely always have to be <code>torch.bfloat16</code> because:</p> <ul> <li>All models currently supported (except Cog-2b) do not work well in FP16 for inference, so training would be broken as well. This can be revisited if it makes sense to train in FP16 for other models added.</li> <li>The <code>accelerate</code> config files default to using \"bf16\", but modifying that would be at the risk of user and assumes they understand the significance of their changes.</li> </ul>"},{"location":"models/attention/","title":"Attention backends","text":"<p>Finetrainers supports multiple attention backends to support different hardware and tradeoff between speed and memory usage. The following attention implementations are supported: - Training:   - If model uses attention masks: <code>flash_varlen</code>, <code>flex</code>, <code>native</code>   - If model does not use attention masks: <code>flash</code>, <code>flex</code>, <code>native</code>, <code>xformers</code> - Inference:   - If model uses attention masks: <code>flash_varlen</code>, <code>flex</code>, <code>native</code>, <code>sage_varlen</code>   - If model does not use attention masks: <code>flash</code>, <code>flash_varlen</code>, <code>flex</code>, <code>native</code>, <code>sage</code>, <code>sage_varlen</code>, <code>xformers</code></p> <p>Additionally, some specialized methods are available for debugging-specific purposes: <code>_native_cudnn</code>, <code>_native_efficient</code>, <code>_native_flash</code>, <code>_native_math</code>, <code>_sage_qk_int8_pv_fp8_cuda</code>, <code>_sage_qk_int8_pv_fp8_cuda_sm90</code>, <code>_sage_qk_int8_pv_fp16_cuda</code>, <code>_sage_qk_int8_pv_fp16_triton</code>. With time, more attention-specific optimizations and custom implementations will be supported. Contributions are welcome!</p> <p>Unfortunately, due to limited time for testing, only specific versions of packages that provide these implementations are supported. Other versions may work. The supported versions will be gradually made lower for more flexibility, but for now, please use the following versions: - <code>flash-attn&gt;=2.6.3</code> - <code>sageattention&gt;=2.1.1</code> - <code>xformers&gt;=0.0.29.post3</code></p> <p>This guide will help you quickly install flash-attn, sageattention, and xformers to make your models run faster and use less memory for training/inference. We'll cover installation on Linux (Ubuntu 22.04) and Windows (using WSL).</p> <p>Before you start, make sure to use a clean python virtual environment to not mess up your system seriously, or to avoid conflicting dependencies leading to failed installations which might leave the environment in hard-to-recover state.</p>"},{"location":"models/attention/#flash-attention","title":"Flash attention","text":"<p>Providers covered: <code>flash</code>, <code>flash_varlen</code></p> <p>The installation steps have only been tested with Ubuntu 22.04; CUDA version higher than 12.2 and 12.6. - Check your CUDA version: look at the output of <code>nvidia-smi</code> or run <code>nvcc --version</code>. - You might need the following packages: <code>pip install packaging ninja</code> - Linux: Run: <code>pip install flash-attn --no-build-isolation</code>. Verify the version with <code>pip show flash-attn</code> - WSL: Same instruction as above should work. Native Windows might require building from source - check community guiders and follow the instruction here.</p>"},{"location":"models/attention/#sage-attention","title":"Sage attention","text":"<p>Providers covered: <code>sage</code>, <code>sage_varlen</code>, <code>_sage_qk_int8_pv_fp8_cuda</code>, <code>_sage_qk_int8_pv_fp8_cuda_sm90</code>, <code>_sage_qk_int8_pv_fp16_cuda</code>, <code>_sage_qk_int8_pv_fp16_triton</code></p> <p>FP8 implementations will require CUDA compute capability of 90 or higher (H100, RTX 5090, etc.). Some may work on compute capability 89 as well (RTX 4090, for example). For FP16 implementations, compute capability of atleast 80 is required (A100, RTX 3090, etc.). For other GPUs, FP16 implementations may or may not work (this is untested by me).</p> <ul> <li>Check your compute capability with the following command:   <pre><code>python -c \"import torch; print(torch.cuda.get_device_capability())\"\n</code></pre></li> <li>Check your CUDA version: look at the output of <code>nvidia-smi</code> or run <code>nvcc --version</code>.</li> <li>You might need the following packages: <code>pip install triton</code>. For Windows, check out the triton-windows project.</li> <li>Linux/WSL: Run: <code>pip install git+https://github.com/thu-ml/SageAttention</code>. Verify the version with <code>pip show sageattention</code>.</li> <li>Make sure to look at the official installation guide in SageAttention too!</li> </ul>"},{"location":"models/attention/#xformers","title":"xformers","text":"<p>Providers covered: <code>xformers</code></p> <ul> <li>Check your CUDA version: look at the output of <code>nvidia-smi</code> or run <code>nvcc --version</code>.</li> <li>Linux/WSL: Run: <code>pip install -U xformers --index-url https://download.pytorch.org/whl/cu126</code> (assuming CUDA 12.6). Verify the version with <code>pip show xformers</code>.</li> <li>Make sure to look at the official installation guide in xformers too!</li> </ul> <p>All other providers are either native PyTorch implementations or require a specific PyTorch version (for example, Flex Attention requires torch version of atleast 2.5.0).</p>"},{"location":"models/attention/#usage","title":"Usage","text":"<p>There are two ways to use the attention dispatcher mechanism: - Replace <code>scaled_dot_product_attention</code> globally:   <pre><code>import torch.nn.functional as F\nfrom finetrainers.models.attention_dispatch import attention_dispatch\n\nF.scaled_dot_product_attention = attention_dispatch\n</code></pre> - Replace all occurrences of <code>scaled_dot_product_attention</code> in your code with <code>attention_dispatch</code>.</p> <pre><code># Use dispatcher directly\nfrom finetrainers.models.attention_dispatch import attention_provider, AttentionProvider\n\nwith attention_provider(AttentionProvider.FLASH_VARLEN):\n    model(...)\n\n# or,\nwith attention_provider(\"sage_varlen\"):\n    model(...)\n</code></pre>"},{"location":"models/attention/#context-parallel","title":"Context Parallel","text":"<p>TODO</p>"},{"location":"models/cogvideox/","title":"CogVideoX","text":""},{"location":"models/cogvideox/#training","title":"Training","text":"<p>For LoRA training, specify <code>--training_type lora</code>. For full finetuning, specify <code>--training_type full-finetune</code>.</p> <p>Examples available: - PIKA crush effect</p> <p>To run an example, run the following from the root directory of the repository (assuming you have installed the requirements and are using Linux/WSL):</p> <pre><code>chmod +x ./examples/training/sft/cogvideox/crush_smol_lora/train.sh\n./examples/training/sft/cogvideox/crush_smol_lora/train.sh\n</code></pre> <p>On Windows, you will have to modify the script to a compatible format to run it. [TODO(aryan): improve instructions for Windows]</p>"},{"location":"models/cogvideox/#supported-checkpoints","title":"Supported checkpoints","text":"<p>CogVideoX has multiple checkpoints as one can note here. The following checkpoints were tested with <code>finetrainers</code> and are known to be working:</p> <ul> <li>THUDM/CogVideoX-2b</li> <li>THUDM/CogVideoX-5B</li> <li>THUDM/CogVideoX1.5-5B</li> </ul>"},{"location":"models/cogvideox/#inference","title":"Inference","text":"<p>Assuming your LoRA is saved and pushed to the HF Hub, and named <code>my-awesome-name/my-awesome-lora</code>, we can now use the finetuned model for inference:</p> <pre><code>import torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n\npipe = CogVideoXPipeline.from_pretrained(\n    \"THUDM/CogVideoX-5b\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n+ pipe.load_lora_weights(\"my-awesome-name/my-awesome-lora\", adapter_name=\"cogvideox-lora\")\n+ pipe.set_adapters([\"cogvideox-lora\"], [0.75])\n\nvideo = pipe(\"&lt;my-awesome-prompt&gt;\").frames[0]\nexport_to_video(video, \"output.mp4\")\n</code></pre> <p>You can refer to the following guides to know more about the model pipeline and performing LoRA inference in <code>diffusers</code>:</p> <ul> <li>CogVideoX in Diffusers</li> <li>Load LoRAs for inference</li> <li>Merge LoRAs</li> </ul>"},{"location":"models/cogview4/","title":"CogView4","text":""},{"location":"models/cogview4/#training","title":"Training","text":"<p>For LoRA training, specify <code>--training_type lora</code>. For full finetuning, specify <code>--training_type full-finetune</code>.</p> <p>Examples available: - Raider White Tarot cards style - Omni Edit Control LoRA - Canny Control LoRA</p> <p>To run an example, run the following from the root directory of the repository (assuming you have installed the requirements and are using Linux/WSL):</p> <pre><code>chmod +x ./examples/training/sft/cogview4/raider_white_tarot/train.sh\n./examples/training/sft/cogview4/raider_white_tarot/train.sh\n</code></pre> <p>On Windows, you will have to modify the script to a compatible format to run it. [TODO(aryan): improve instructions for Windows]</p>"},{"location":"models/cogview4/#supported-checkpoints","title":"Supported checkpoints","text":"<p>The following checkpoints were tested with <code>finetrainers</code> and are known to be working:</p> <ul> <li>THUDM/CogView4-6B</li> </ul>"},{"location":"models/cogview4/#inference","title":"Inference","text":"<p>Assuming your LoRA is saved and pushed to the HF Hub, and named <code>my-awesome-name/my-awesome-lora</code>, we can now use the finetuned model for inference:</p> <pre><code>import torch\nfrom diffusers import CogView4Pipeline\nfrom diffusers.utils import export_to_video\n\npipe = CogView4Pipeline.from_pretrained(\n    \"THUDM/CogView4-6B\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n+ pipe.load_lora_weights(\"my-awesome-name/my-awesome-lora\", adapter_name=\"cogview4-lora\")\n+ pipe.set_adapters([\"cogview4-lora\"], [0.9])\n\nvideo = pipe(\"&lt;my-awesome-prompt&gt;\").frames[0]\nexport_to_video(video, \"output.mp4\")\n</code></pre> <p>To use trained Control LoRAs, the following can be used for inference (ideally, you should raise a support request in Diffusers):</p>  Control Lora inference  <pre><code>import torch\nfrom diffusers import CogView4Pipeline\nfrom diffusers.utils import load_image\nfrom finetrainers.models.utils import _expand_linear_with_zeroed_weights\nfrom finetrainers.patches import load_lora_weights\nfrom finetrainers.patches.dependencies.diffusers.control import control_channel_concat\n\ndtype = torch.bfloat16\ndevice = torch.device(\"cuda\")\ngenerator = torch.Generator().manual_seed(0)\n\npipe = CogView4Pipeline.from_pretrained(\"THUDM/CogView4-6B\", torch_dtype=dtype)\n\nin_channels = pipe.transformer.config.in_channels\npatch_channels = pipe.transformer.patch_embed.proj.in_features\npipe.transformer.patch_embed.proj = _expand_linear_with_zeroed_weights(pipe.transformer.patch_embed.proj, new_in_features=2 * patch_channels)\n\nload_lora_weights(pipe, \"/raid/aryan/cogview4-control-lora\", \"cogview4-lora\")\npipe.to(device)\n\nprompt = \"Make the image look like it's from an ancient Egyptian mural.\"\ncontrol_image = load_image(\"examples/training/control/cogview4/omni_edit/validation_dataset/0.png\")\nheight, width = 1024, 1024\n\nwith torch.no_grad():\n    latents = pipe.prepare_latents(1, in_channels, height, width, dtype, device, generator)\n    control_image = pipe.image_processor.preprocess(control_image, height=height, width=width)\n    control_image = control_image.to(device=device, dtype=dtype)\n    control_latents = pipe.vae.encode(control_image).latent_dist.sample(generator=generator)\n    control_latents = (control_latents - pipe.vae.config.shift_factor) * pipe.vae.config.scaling_factor\n\nwith control_channel_concat(pipe.transformer, [\"hidden_states\"], [control_latents], dims=[1]):\n    image = pipe(prompt, latents=latents, num_inference_steps=30, generator=generator).images[0]\n\nimage.save(\"output.png\")\n</code></pre> <p>You can refer to the following guides to know more about the model pipeline and performing LoRA inference in <code>diffusers</code>:</p> <ul> <li>CogView4 in Diffusers</li> <li>Load LoRAs for inference</li> <li>Merge LoRAs</li> </ul>"},{"location":"models/flux/","title":"Flux","text":""},{"location":"models/flux/#training","title":"Training","text":"<p>For LoRA training, specify <code>--training_type lora</code>. For full finetuning, specify <code>--training_type full-finetune</code>.</p> <p>Examples available: - Raider White Tarot cards style</p> <p>To run an example, run the following from the root directory of the repository (assuming you have installed the requirements and are using Linux/WSL):</p> <pre><code>chmod +x ./examples/training/sft/flux_dev/raider_white_tarot/train.sh\n./examples/training/sft/flux_dev/raider_white_tarot/train.sh\n</code></pre> <p>On Windows, you will have to modify the script to a compatible format to run it. [TODO(aryan): improve instructions for Windows]</p> <p>Warning</p> <p>Currently, only FLUX.1-dev is supported. It is a guidance-distilled model which directly predicts the outputs of its teacher model when the teacher is run with CFG. To match the output distribution of the distilled model with that of the teacher model, a guidance scale of 1.0 is hardcoded into the codebase. However, other values may work too but it is experimental.</p> <p>Info</p> <p>FLUX.1-schnell is not supported for training yet. It is a timestep-distilled model. Matching its output distribution for training is significantly more difficult.</p>"},{"location":"models/flux/#supported-checkpoints","title":"Supported checkpoints","text":"<p>The following checkpoints were tested with <code>finetrainers</code> and are known to be working:</p> <ul> <li>black-forest-labs/FLUX.1-dev</li> <li>black-forest-labs/FLUX.1-schnell</li> </ul>"},{"location":"models/flux/#inference","title":"Inference","text":"<p>Assuming your LoRA is saved and pushed to the HF Hub, and named <code>my-awesome-name/my-awesome-lora</code>, we can now use the finetuned model for inference:</p> <pre><code>import torch\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n+ pipe.load_lora_weights(\"my-awesome-name/my-awesome-lora\", adapter_name=\"flux-lora\")\n+ pipe.set_adapters([\"flux-lora\"], [0.9])\n\n# Make sure to set guidance_scale to 0.0 when inferencing with FLUX.1-schnell or derivative models\nimage = pipe(\"&lt;my-awesome-prompt&gt;\").images[0]\nimage.save(\"output.png\")\n</code></pre> <p>You can refer to the following guides to know more about the model pipeline and performing LoRA inference in <code>diffusers</code>:</p> <ul> <li>Flux in Diffusers</li> <li>Load LoRAs for inference</li> <li>Merge LoRAs</li> </ul>"},{"location":"models/hunyuan_video/","title":"HunyuanVideo","text":""},{"location":"models/hunyuan_video/#training","title":"Training","text":"<p>For LoRA training, specify <code>--training_type lora</code>. For full finetuning, specify <code>--training_type full-finetune</code>.</p> <p>Examples available: - PIKA Dissolve effect</p> <p>To run an example, run the following from the root directory of the repository (assuming you have installed the requirements and are using Linux/WSL):</p> <pre><code>chmod +x ./examples/training/sft/hunyuan_video/modal_labs_dissolve/train.sh\n./examples/training/sft/hunyuan_video/modal_labs_dissolve/train.sh\n</code></pre> <p>On Windows, you will have to modify the script to a compatible format to run it. [TODO(aryan): improve instructions for Windows]</p>"},{"location":"models/hunyuan_video/#inference","title":"Inference","text":"<p>Assuming your LoRA is saved and pushed to the HF Hub, and named <code>my-awesome-name/my-awesome-lora</code>, we can now use the finetuned model for inference:</p> <pre><code>import torch\nfrom diffusers import HunyuanVideoPipeline\n\nimport torch\nfrom diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"hunyuanvideo-community/HunyuanVideo\"\ntransformer = HunyuanVideoTransformer3DModel.from_pretrained(\n    model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16\n)\npipe = HunyuanVideoPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.float16)\npipe.load_lora_weights(\"my-awesome-name/my-awesome-lora\", adapter_name=\"hunyuanvideo-lora\")\npipe.set_adapters([\"hunyuanvideo-lora\"], [0.6])\npipe.vae.enable_tiling()\npipe.to(\"cuda\")\n\noutput = pipe(\n    prompt=\"A cat walks on the grass, realistic\",\n    height=320,\n    width=512,\n    num_frames=61,\n    num_inference_steps=30,\n).frames[0]\nexport_to_video(output, \"output.mp4\", fps=15)\n</code></pre> <p>You can refer to the following guides to know more about the model pipeline and performing LoRA inference in <code>diffusers</code>:</p> <ul> <li>Hunyuan-Video in Diffusers</li> <li>Load LoRAs for inference</li> <li>Merge LoRAs</li> </ul>"},{"location":"models/ltx_video/","title":"LTX-Video","text":""},{"location":"models/ltx_video/#training","title":"Training","text":"<p>For LoRA training, specify <code>--training_type lora</code>. For full finetuning, specify <code>--training_type full-finetune</code>.</p> <p>Examples available: - PIKA crush effect</p> <p>To run an example, run the following from the root directory of the repository (assuming you have installed the requirements and are using Linux/WSL):</p> <pre><code>chmod +x ./examples/training/sft/ltx_video/crush_smol_lora/train.sh\n./examples/training/sft/ltx_video/crush_smol_lora/train.sh\n</code></pre> <p>On Windows, you will have to modify the script to a compatible format to run it. [TODO(aryan): improve instructions for Windows]</p>"},{"location":"models/ltx_video/#inference","title":"Inference","text":"<p>Assuming your LoRA is saved and pushed to the HF Hub, and named <code>my-awesome-name/my-awesome-lora</code>, we can now use the finetuned model for inference:</p> <pre><code>import torch\nfrom diffusers import LTXPipeline\nfrom diffusers.utils import export_to_video\n\npipe = LTXPipeline.from_pretrained(\n    \"Lightricks/LTX-Video\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n+ pipe.load_lora_weights(\"my-awesome-name/my-awesome-lora\", adapter_name=\"ltxv-lora\")\n+ pipe.set_adapters([\"ltxv-lora\"], [0.75])\n\nvideo = pipe(\"&lt;my-awesome-prompt&gt;\").frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\n</code></pre> <p>You can refer to the following guides to know more about the model pipeline and performing LoRA inference in <code>diffusers</code>:</p> <ul> <li>LTX-Video in Diffusers</li> <li>Load LoRAs for inference</li> <li>Merge LoRAs</li> </ul>"},{"location":"models/optimization/","title":"Memory optimizations","text":"<p>To lower memory requirements during training:</p> <ul> <li><code>--precompute_conditions</code>: this precomputes the conditions and latents, and loads them as required during training, which saves a significant amount of time and memory.</li> <li><code>--gradient_checkpointing</code>: this saves memory by recomputing activations during the backward pass.</li> <li><code>--layerwise_upcasting_modules transformer</code>: naively casts the model weights to <code>torch.float8_e4m3fn</code> or <code>torch.float8_e5m2</code>. This halves the memory requirement for model weights. Computation is performed in the dtype set by <code>--transformer_dtype</code> (which defaults to <code>bf16</code>)</li> <li><code>--use_8bit_bnb</code>: this is only applicable to Adam and AdamW optimizers, and makes use of 8-bit precision to store optimizer states.</li> <li>Use a DeepSpeed config to launch training (refer to <code>accelerate_configs/deepspeed.yaml</code> as an example).</li> <li>Do not perform validation/testing. This saves a significant amount of memory, which can be used to focus solely on training if you're on smaller VRAM GPUs.</li> </ul> <p>We will continue to add more features that help to reduce memory consumption.</p>"},{"location":"models/wan/","title":"Wan","text":""},{"location":"models/wan/#training","title":"Training","text":"<p>For LoRA training, specify <code>--training_type lora</code>. For full finetuning, specify <code>--training_type full-finetune</code>.</p> <p>Examples available: - PIKA crush effect - 3DGS dissolve - I2V conditioning</p> <p>To run an example, run the following from the root directory of the repository (assuming you have installed the requirements and are using Linux/WSL):</p> <pre><code>chmod +x ./examples/training/sft/wan/crush_smol_lora/train.sh\n./examples/training/sft/wan/crush_smol_lora/train.sh\n</code></pre> <p>On Windows, you will have to modify the script to a compatible format to run it. </p> Info <p>[TODO(aryan): improve instructions for Windows]</p>"},{"location":"models/wan/#supported-checkpoints","title":"Supported checkpoints","text":"<p>Wan has multiple checkpoints as one can find here. The following checkpoints were tested with <code>finetrainers</code> and are known to be working:</p> <ul> <li>Wan-AI/Wan2.1-T2V-1.3B-Diffusers</li> <li>Wan-AI/Wan2.1-T2V-14B-Diffusers</li> <li>Wan-AI/Wan2.1-I2V-14B-480P-Diffusers</li> <li>Wan-AI/Wan2.1-I2V-14B-720P-Diffusers</li> <li>Wan-AI/Wan2.1-FLF2V-14B-720P-diffusers</li> </ul>"},{"location":"models/wan/#inference","title":"Inference","text":"<p>Assuming your LoRA is saved and pushed to the HF Hub, and named <code>my-awesome-name/my-awesome-lora</code>, we can now use the finetuned model for inference:</p> <pre><code>import torch\nfrom diffusers import WanPipeline\nfrom diffusers.utils import export_to_video\n\npipe = WanPipeline.from_pretrained(\n    \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n+ pipe.load_lora_weights(\"my-awesome-name/my-awesome-lora\", adapter_name=\"wan-lora\")\n+ pipe.set_adapters([\"wan-lora\"], [0.75])\n\nvideo = pipe(\"&lt;my-awesome-prompt&gt;\").frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\n</code></pre> <p>To use trained Control LoRAs, the following can be used for inference (ideally, you should raise a support request in Diffusers):</p>  Control Lora inference  <pre><code>import numpy as np\nimport torch\nfrom diffusers import WanPipeline\nfrom diffusers.utils import export_to_video, load_video\nfrom finetrainers.trainer.control_trainer.data import apply_frame_conditioning_on_latents\nfrom finetrainers.models.utils import _expand_conv3d_with_zeroed_weights\nfrom finetrainers.patches import load_lora_weights\nfrom finetrainers.patches.dependencies.diffusers.control import control_channel_concat\n\ndtype = torch.bfloat16\ndevice = torch.device(\"cuda\")\ngenerator = torch.Generator().manual_seed(0)\n\npipe = WanPipeline.from_pretrained(\"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", torch_dtype=dtype).to(device)\n\nin_channels = pipe.transformer.config.in_channels\npatch_channels = pipe.transformer.patch_embedding.in_channels\npipe.transformer.patch_embedding = _expand_conv3d_with_zeroed_weights(pipe.transformer.patch_embedding, new_in_channels=2 * patch_channels)\n\nload_lora_weights(pipe, \"/raid/aryan/wan-control-image-condition\", \"wan-lora\")\npipe.to(device)\n\nprompt = \"The video shows a vibrant green Mustang GT parked in a parking lot. The car is positioned at an angle, showcasing its sleek design and black rims. The car's hood is black, contrasting with the green body. The Mustang GT logo is visible on the side of the car. The parking lot appears to be empty, with the car being the main focus of the video. The car's position and the absence of other vehicles suggest that the video might be a promotional or showcase video for the Mustang GT. The overall style of the video is simple and straightforward, focusing on the car and its design.\"\ncontrol_video = load_video(\"examples/training/control/wan/image_condition/validation_dataset/0.mp4\")\nheight, width, num_frames = 480, 704, 49\n\n# Take evenly space `num_frames` frames from the control video\nindices = np.linspace(0, len(control_video) - 1, num_frames).astype(int)\ncontrol_video = [control_video[i] for i in indices]\n\nwith torch.no_grad():\n    latents = pipe.prepare_latents(1, in_channels, height, width, num_frames, dtype, device, generator)\n    latents_mean = torch.tensor(pipe.vae.config.latents_mean).view(1, -1, 1, 1, 1).to(latents)\n    latents_std = 1.0 / torch.tensor(pipe.vae.config.latents_std).view(1, -1, 1, 1, 1).to(latents)\n    control_video = pipe.video_processor.preprocess_video(control_video, height=height, width=width)\n    control_video = control_video.to(device=device, dtype=dtype)\n    control_latents = pipe.vae.encode(control_video).latent_dist.sample(generator=generator)\n    control_latents = ((control_latents.float() - latents_mean) * latents_std).to(dtype)\n    control_latents = apply_frame_conditioning_on_latents(\n        control_latents,\n        expected_num_frames=latents.size(2),\n        channel_dim=1,\n        frame_dim=2,\n        frame_conditioning_type=\"index\",\n        frame_conditioning_index=0,\n        concatenate_mask=False,\n    )\n\nwith control_channel_concat(pipe.transformer, [\"hidden_states\"], [control_latents], dims=[1]):\n    video = pipe(prompt, latents=latents, num_inference_steps=30, generator=generator).frames[0]\n\nexport_to_video(video, \"output.mp4\", fps=16)\n</code></pre> <p>You can refer to the following guides to know more about the model pipeline and performing LoRA inference in <code>diffusers</code>:</p> <ul> <li>Wan in Diffusers</li> <li>Load LoRAs for inference</li> <li>Merge LoRAs</li> </ul>"},{"location":"parallel/","title":"Finetrainers Parallel Backends","text":"<p>Finetrainers supports parallel training on multiple GPUs &amp; nodes. This is done using the Pytorch DTensor backend. To run parallel training, <code>torchrun</code> is utilized.</p> <p>As an experiment for comparing performance of different training backends, Finetrainers has implemented multi-backend support. These backends may or may not fully rely on Pytorch's distributed DTensor solution. Currently, only \ud83e\udd17 Accelerate is supported for backwards-compatibility reasons (as we initially started Finetrainers with only Accelerate). In the near future, there are plans for integrating with: - DeepSpeed - Nanotron - Megatron-LM</p> <p>Warning</p> <p>The multi-backend support is completely experimental and only serves to satisfy my curiosity of how much of a tradeoff there is between performance and ease of use. The Pytorch DTensor backend is the only one with stable support, following Accelerate.</p> <p>Users will not have to worry about backwards-breaking changes or dependencies if they stick to the Pytorch DTensor backend.</p>"},{"location":"parallel/#support-matrix","title":"Support matrix","text":"<p>There are various algorithms for parallel training. Currently, we only support: - DDP - FSDP2 - HSDP - TP</p>"},{"location":"parallel/#training","title":"Training","text":"<p>The following parameters are relevant for launching training:</p> <ul> <li><code>parallel_backend</code>: The backend to use for parallel training. Available options are <code>ptd</code> &amp; <code>accelerate</code>.</li> <li><code>pp_degree</code>: The degree of pipeline parallelism. Currently unsupported.</li> <li><code>dp_degree</code>: The degree of data parallelis/replicas. Defaults to <code>1</code>.</li> <li><code>dp_shards</code>: The number of shards for data parallelism. Defaults to <code>1</code>.</li> <li><code>cp_degree</code>: The degree of context parallelism. Currently unsupported.</li> <li><code>tp_degree</code>: The degree of tensor parallelism.</li> </ul> <p>For launching training with the Pytorch DTensor backend, use the following command:</p> <pre><code># Single node - 8 GPUs available\ntorchrun --standalone --nodes=1 --nproc_per_node=8 --rdzv_backend c10d --rdzv_endpoint=\"localhost:0\" train.py &lt;YOUR_OTHER_ARGS&gt;\n\n# Single node - 8 GPUs but only 4 available\nexport CUDA_VISIBLE_DEVICES=0,2,4,5\ntorchrun --standalone --nodes=1 --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=\"localhost:0\" train.py &lt;YOUR_OTHER_ARGS&gt;\n\n# Multi-node - Nx8 GPUs available\n# TODO(aryan): Add slurm script\n</code></pre> <p>For launching training with the Accelerate backend, use the following command:</p> <pre><code># Single node - 8 GPUs available\naccelerate launch --config_file accelerate_configs/uncompiled_8.yaml --gpu_ids 0,1,2,3,4,5,6,7 train.py &lt;YOUR_OTHER_ARGS&gt;\n\n# Single node - 8 GPUs but only 4 available\naccelerate launch --config_file accelerate_configs/uncompiled_4.yaml --gpu_ids 0,2,4,5 train.py &lt;YOUR_OTHER_ARGS&gt;\n\n# Multi-node - Nx8 GPUs available\n# TODO(aryan): Add slurm script\n</code></pre>"},{"location":"trainer/control_trainer/","title":"Control Trainer","text":"<p>The Control trainer supports channel-concatenated control conditioning for models either using low-rank adapters or full-rank training. It involves adding extra input channels to the patch embedding layer (referred to as the \"control injection\" layer in finetrainers), to mix conditioning features into the latent stream. This architecture choice is very common and has been seen before in many models - CogVideoX-I2V, HunyuanVideo-I2V, Alibaba's Fun Control models, etc.</p>"},{"location":"trainer/sft_trainer/","title":"SFT Trainer","text":"<p>The SFT trainer supports low-rank and full-rank finetuning of models.</p>"}]}